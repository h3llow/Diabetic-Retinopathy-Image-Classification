{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!pip install transformers datasets tensorflow_datasets opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import keras.optimizers\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "from transformers import AutoImageProcessor, ViTImageProcessor\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import cv2\n",
    "import datasets\n",
    "from transformers import DefaultDataCollator\n",
    "from transformers import TFViTForImageClassification, create_optimizer, TFCvtForImageClassification\n",
    "from transformers import CvtConfig, CvtModel\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sn\n",
    "\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def create_image_folder_dataset(root_path):\n",
    "    \"\"\"creates `Dataset` from image folder structure\"\"\"\n",
    "\n",
    "    # get class names by folders names\n",
    "    _CLASS_NAMES = os.listdir(root_path)\n",
    "    # defines `datasets` features`\n",
    "    features = datasets.Features({\n",
    "        \"img\": datasets.Image(),\n",
    "        \"label\": datasets.features.ClassLabel(names=_CLASS_NAMES),\n",
    "    })\n",
    "    # temp list holding datapoints for creation\n",
    "    img_data_files = []\n",
    "    label_data_files = []\n",
    "    # load images into list for creation\n",
    "    for img_class in os.listdir(root_path):\n",
    "        for img in os.listdir(os.path.join(root_path, img_class)):\n",
    "            path_ = os.path.join(root_path, img_class, img)\n",
    "            img_data_files.append(path_)\n",
    "            label_data_files.append(img_class)\n",
    "    # create dataset\n",
    "    ds = datasets.Dataset.from_dict({\"img\": img_data_files, \"label\": label_data_files}, features=features)\n",
    "    return ds\n",
    "\n",
    "\n",
    "train_imgs = create_image_folder_dataset(\"drive/MyDrive/project1/train\")\n",
    "img_class_labels = train_imgs.features[\"label\"].names"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_id = \"google/vit-base-patch16-224-in21k\"\n",
    "#model_id = \"microsoft/cvt-13\"\n",
    "feature_extractor = ViTImageProcessor.from_pretrained(model_id)\n",
    "#feature_extractor.size = {\"shortest_edge\":512,}\n",
    "# learn more about data augmentation here: https://www.tensorflow.org/tutorials/images/data_augmentation\n",
    "data_augmentation = keras.Sequential(\n",
    "    [\n",
    "        layers.Resizing(1024, 1024),\n",
    "        layers.CenterCrop(900, 900),\n",
    "        layers.experimental.preprocessing.RandomCrop(300, 300),\n",
    "        layers.RandomBrightness(factor=0.2),\n",
    "        layers.RandomContrast(factor=0.2),\n",
    "\n",
    "        layers.Rescaling(1/255),\n",
    "        layers.Resizing(224, 224),\n",
    "        #layers.RandomZoom(height_factor=(0,0.15), width_factor=(0,0.15), fill_mode=\"constant\", ),\n",
    "        layers.RandomFlip(\"horizontal_and_vertical\"),\n",
    "\n",
    "        layers.RandomRotation(factor=0.2, fill_mode=\"constant\", fill_value=0),\n",
    "    ],\n",
    "    name=\"data_augmentation\",\n",
    ")\n",
    "data_resizing = keras.Sequential(\n",
    "    [\n",
    "        layers.Resizing(1024, 1024),\n",
    "        layers.CenterCrop(900, 900),\n",
    "        layers.experimental.preprocessing.RandomCrop(450, 450),\n",
    "        layers.Rescaling(1/255),\n",
    "        layers.Resizing(224, 224),\n",
    "    ],\n",
    "    name=\"data_resizing\",\n",
    ")\n",
    "def load_ben_color(image):\n",
    "    sigmaX=10\n",
    "    #image = cv2.resize(image, (1024, 1024))\n",
    "    image=cv2.addWeighted(image, 4, cv2.GaussianBlur(image, (0,0), sigmaX) ,-4 ,128)\n",
    "    return image\n",
    "# use keras image data augementation processing\n",
    "def augmentation(examples):\n",
    "\n",
    "    inputs = {\"pixel_values\":[data_augmentation(np.array(load_ben_color(np.array(img)))) for img in examples['img']], \"labels\":examples[\"label\"]}\n",
    "    inputs[\"pixel_values\"] = np.array(inputs[\"pixel_values\"]).swapaxes(1,3)\n",
    "    #raise Exception(str(tf.reduce_min(inputs[\"pixel_values\"])) + \" \" + str(tf.reduce_max(inputs[\"pixel_values\"])))\n",
    "\n",
    "    return inputs\n",
    "\n",
    "\n",
    "# basic processing (only resizing)\n",
    "def process(examples):\n",
    "    inputs = {\"pixel_values\":[data_resizing(np.array(load_ben_color(np.array(img)))) for img in examples['img']], \"labels\":examples[\"label\"]}\n",
    "    inputs[\"pixel_values\"] = np.array(inputs[\"pixel_values\"]).swapaxes(1,3)\n",
    "    return inputs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_size = .1\n",
    "train_val_set = train_imgs.train_test_split(test_size=test_size)\n",
    "train_val_set[\"test\"] = train_val_set[\"test\"].with_transform(process)\n",
    "train_val_set[\"train\"] = train_val_set[\"train\"].with_transform(augmentation)\n",
    "#train_val_set[\"train\"] = train_val_set[\"train\"].with_transform(augmentation)\n",
    "\n",
    "from transformers import TFViTForImageClassification\n",
    "\n",
    "labels = train_val_set['train'].features['label'].names\n",
    "\n",
    "model = TFViTForImageClassification.from_pretrained(\n",
    "    model_id,\n",
    "    num_labels=len(labels),\n",
    "    id2label={str(i): c for i, c in enumerate(labels)},\n",
    "    label2id={c: str(i) for i, c in enumerate(labels)}\n",
    ")\n",
    "\n",
    "#model = TFCvtForImageClassification.from_pretrained(\n",
    "#    model_id,\n",
    "#    )\n",
    "#model.classifier = tf.keras.layers.Dense(5)\n",
    "#model.num_labels = 5"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "num_train_epochs = 5\n",
    "train_batch_size = 32\n",
    "eval_batch_size = 32\n",
    "learning_rate = 0.00003\n",
    "weight_decay_rate = 0.01\n",
    "num_warmup_steps = 0\n",
    "output_dir = model_id.split(\"/\")[1]\n",
    "hub_model_id = f'{model_id.split(\"/\")[1]}-eyes'\n",
    "fp16 = True\n",
    "data_collator = DefaultDataCollator(return_tensors=\"tf\")\n",
    "\n",
    "# converting our train dataset to tf.data.Dataset\n",
    "tf_train_dataset = train_val_set[\"train\"].to_tf_dataset(\n",
    "    columns=['pixel_values'],\n",
    "    label_cols=[\"labels\"],\n",
    "    shuffle=True,\n",
    "    batch_size=train_batch_size,\n",
    "    collate_fn=data_collator)\n",
    "\n",
    "# converting our test dataset to tf.data.Dataset\n",
    "tf_eval_dataset = train_val_set[\"test\"].to_tf_dataset(\n",
    "    columns=['pixel_values'],\n",
    "    label_cols=[\"labels\"],\n",
    "    shuffle=True,\n",
    "    batch_size=eval_batch_size,\n",
    "    collate_fn=data_collator)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"for x, y in tf_train_dataset:\n",
    "    print(np.array(x[0]).min(), np.array(x[0]).max())\n",
    "    test = np.array(x[0])\n",
    "    plt.imshow(test.swapaxes(0,2))\n",
    "    plt.show()\"\"\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class model_per_epoch(keras.callbacks.Callback):\n",
    "    def __init__(self, model,filepath,save_best_only):\n",
    "        self.filepath=filepath\n",
    "        self.model=model\n",
    "        self.save_best_only=save_best_only\n",
    "        self.lowest_loss=np.inf\n",
    "        self.best_weights=self.model.get_weights()\n",
    "    def on_epoch_end(self,epoch, logs=None):\n",
    "        v_loss=logs.get('val_loss')\n",
    "        if v_loss< self.lowest_loss:\n",
    "            self.lowest_loss =v_loss\n",
    "            self.best_weights=self.model.get_weights()\n",
    "            self.best_epoch=epoch +1\n",
    "            self.model.set_weights(self.best_weights)\n",
    "            name= str(self.best_epoch) +'-' + str(self.lowest_loss)[:str(self.lowest_loss).rfind('.')+3] + '.h5'\n",
    "            file_id=os.path.join(self.filepath, name)\n",
    "            self.model.save_weights(file_id)\n",
    "        if self.save_best_only==False:\n",
    "            name= str(epoch) +'-' + str(v_loss)[:str(v_loss).rfind('.')+3] + '.h5'\n",
    "            file_id=os.path.join(self.filepath, name)\n",
    "            self.model.save(file_id)\n",
    "    def on_train_end(self, logs=None):\n",
    "        if self.save_best_only == True:\n",
    "            self.model.set_weights(self.best_weights)\n",
    "            name= str(self.best_epoch) +'-' + str(self.lowest_loss)[:str(self.lowest_loss).rfind('.')+3] + '.h5'\n",
    "            file_id=os.path.join(self.filepath, name)\n",
    "            self.model.save_weights(file_id)\n",
    "            print(' model is returned with best weights from epoch ', self.best_epoch)\n",
    "\n",
    "save_dir=r'drive/MyDrive/project1/'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "num_train_steps = len(tf_train_dataset) * num_train_epochs\n",
    "optimizer, lr_schedule = create_optimizer(\n",
    "    init_lr=learning_rate,\n",
    "    num_train_steps=num_train_steps,\n",
    "    weight_decay_rate=weight_decay_rate,\n",
    "    num_warmup_steps=num_warmup_steps,\n",
    ")\n",
    "\n",
    "# define metrics\n",
    "metrics=[\n",
    "    tf.keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),\n",
    "]\n",
    "callbacks=[tf.keras.callbacks.EarlyStopping(patience=2),\n",
    "           model_per_epoch(model, save_dir, True)]\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss=loss,\n",
    "              metrics=metrics,)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    tf_train_dataset.prefetch(20),\n",
    "    validation_data=tf_eval_dataset.prefetch(20),\n",
    "    callbacks=callbacks,\n",
    "    epochs=100,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_imgs = create_image_folder_dataset(\"drive/MyDrive/project1/test\")\n",
    "test_imgs = test_imgs.with_transform(process)\n",
    "test_imgs = test_imgs.to_tf_dataset(\n",
    "    columns=['pixel_values'],\n",
    "    label_cols=[\"labels\"],\n",
    "    shuffle=False,\n",
    "    batch_size=1,\n",
    "    collate_fn=data_collator)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_pred = model.predict(test_imgs)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_pred_ensemble = np.empty((y_pred.shape[0],10))\n",
    "y_pred_ensemble[:,0] = y_pred\n",
    "for i in range(1,10):\n",
    "  y_pred_ensemble[:,i] = np.argmax(model.predict(test_imgs).logits, axis=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_pred = []\n",
    "for row in y_pred_ensemble.astype(\"int64\"):\n",
    "  y_pred.append(np.argmax(np.bincount(row)))\n",
    "y_pred = np.array(y_pred)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "testset = list(test_imgs)\n",
    "y = np.concatenate([testset[n][1] for n in range(0, len(testset))])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ax = sn.heatmap(confusion_matrix(y,y_pred), xticklabels=[0, 1, 2, 3, 4], yticklabels=[0, 1, 2, 3, 4], annot=True)\n",
    "ax.set(xlabel=\"True Class\", ylabel=\"Predicted Class\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.save_weights(\"drive/MyDrive/project1/transformer_weights.h5\")"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
