{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import keras.optimizers\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "from transformers import AutoImageProcessor, ViTImageProcessor\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import cv2\n",
    "import datasets\n",
    "from transformers import DefaultDataCollator\n",
    "from transformers import TFViTForImageClassification, create_optimizer, TFCvtForImageClassification\n",
    "from transformers import CvtConfig, CvtModel\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-23T11:44:53.827233500Z",
     "start_time": "2023-11-23T11:44:51.081424800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def create_image_folder_dataset(root_path, processed_path):\n",
    "    \"\"\"creates `Dataset` from image folder structure\"\"\"\n",
    "\n",
    "    # get class names by folders names\n",
    "    _CLASS_NAMES = os.listdir(root_path)\n",
    "    # defines `datasets` features`\n",
    "    features = datasets.Features({\n",
    "        \"img\": datasets.Image(),\n",
    "        \"label\": datasets.features.ClassLabel(names=_CLASS_NAMES),\n",
    "    })\n",
    "    # temp list holding datapoints for creation\n",
    "    img_data_files = []\n",
    "    label_data_files = []\n",
    "    # load images into list for creation\n",
    "    for img_class in os.listdir(root_path):\n",
    "        for img in os.listdir(os.path.join(root_path, img_class)):\n",
    "            os.makedirs(processed_path + \"/\" + img_class, exist_ok=True)\n",
    "            path_ = os.path.join(root_path, img_class, img)\n",
    "            img_data_files.append(path_)\n",
    "            label_data_files.append(img_class)\n",
    "    # create dataset\n",
    "    ds = datasets.Dataset.from_dict({\"img\": img_data_files, \"label\": label_data_files}, features=features)\n",
    "    return ds\n",
    "\n",
    "\n",
    "train_imgs = create_image_folder_dataset(\"train\", \"train_preprocessed\")\n",
    "img_class_labels = train_imgs.features[\"label\"].names"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-23T11:44:54.033535700Z",
     "start_time": "2023-11-23T11:44:53.828237Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'os' has no attribute 'make'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[3], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mos\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmake\u001B[49m\n",
      "\u001B[1;31mAttributeError\u001B[0m: module 'os' has no attribute 'make'"
     ]
    }
   ],
   "source": [
    "os.make"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-23T11:44:54.584854100Z",
     "start_time": "2023-11-23T11:44:54.034540400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_id = \"google/vit-base-patch16-224-in21k\"\n",
    "#model_id = \"microsoft/cvt-13\"\n",
    "feature_extractor = ViTImageProcessor.from_pretrained(model_id)\n",
    "feature_extractor.size = {\"width\":224,\"height\":224}\n",
    "# learn more about data augmentation here: https://www.tensorflow.org/tutorials/images/data_augmentation\n",
    "data_augmentation = keras.Sequential(\n",
    "    [\n",
    "        layers.Rescaling(1/255),\n",
    "        #layers.Resizing(224, 224),\n",
    "        layers.RandomZoom(height_factor=(0,0.15), width_factor=(0,0.15), fill_mode=\"constant\", ),\n",
    "        layers.RandomFlip(\"horizontal\"),\n",
    "        layers.RandomRotation(factor=0.2, fill_mode=\"constant\", fill_value=0),\n",
    "        #layers.RandomBrightness(factor=0.2),\n",
    "        layers.RandomContrast(factor=0.2)\n",
    "    ],\n",
    "    name=\"data_augmentation\",\n",
    ")\n",
    "data_resizing = keras.Sequential(\n",
    "    [\n",
    "        layers.Rescaling(1/255),\n",
    "        #layers.Resizing(224, 224),\n",
    "    ],\n",
    "    name=\"data_resizing\",\n",
    ")\n",
    "def load_ben_color(image):\n",
    "    sigmaX=10\n",
    "    image = cv2.resize(image, (224, 224))\n",
    "    image=cv2.addWeighted(image, 4, cv2.GaussianBlur(image, (0,0), sigmaX) ,-4 ,128)\n",
    "    return image\n",
    "# use keras image data augementation processing\n",
    "def augmentation(examples):\n",
    "\n",
    "    inputs = {\"pixel_values\":[data_augmentation(np.array(load_ben_color(np.array(img)))) for img in examples['img']], \"labels\":examples[\"label\"]}\n",
    "    #inputs[\"pixel_values\"] = np.array(inputs[\"pixel_values\"]).swapaxes(1,3)\n",
    "    #raise Exception(str(tf.reduce_min(inputs[\"pixel_values\"])) + \" \" + str(tf.reduce_max(inputs[\"pixel_values\"])))\n",
    "\n",
    "    return inputs\n",
    "\n",
    "\n",
    "# basic processing (only resizing)\n",
    "def process(examples):\n",
    "    inputs = {\"pixel_values\":[data_resizing(np.array(load_ben_color(np.array(img)))) for img in examples['img']], \"labels\":examples[\"label\"]}\n",
    "    #inputs[\"pixel_values\"] = np.array(inputs[\"pixel_values\"]).swapaxes(1,3)\n",
    "    return inputs"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-23T11:44:54.586861200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_size = .1\n",
    "train_val_set = train_imgs.train_test_split(test_size=test_size)\n",
    "train_val_set[\"test\"] = train_val_set[\"test\"].with_transform(process)\n",
    "train_val_set[\"train\"] = datasets.concatenate_datasets([train_val_set[\"train\"]]).with_transform(augmentation)\n",
    "#train_val_set[\"train\"] = train_val_set[\"train\"].with_transform(augmentation)\n",
    "\n",
    "from transformers import TFViTForImageClassification\n",
    "\n",
    "labels = train_val_set['train'].features['label'].names\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
    "\n",
    "# ResNet50 model\n",
    "#resnet_50 = ResNet50(include_top=False, weights=\"imagenet\", input_shape=(224, 224, 3))\n",
    "resnet_50 = ResNet50(weights='imagenet',\n",
    "                   include_top=False,\n",
    "                   input_shape=(224, 224, 3))\n",
    "#resnet_50.trainable = False\n",
    "#build the entire model\n",
    "x = resnet_50.output\n",
    "x = layers.GlobalMaxPooling2D()(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Dropout(0.2)(x)\n",
    "x = layers.Dense(1024, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.1)(x)\n",
    "x = layers.Dense(64, activation=\"relu\")(x)\n",
    "predictions = layers.Dense(5, activation='softmax')(x)\n",
    "model = Model(inputs=resnet_50.input, outputs=predictions)\n",
    "for layer in resnet_50.layers[:-20]:\n",
    "   layer.trainable = False\n",
    "\"\"\"model = TFViTForImageClassification.from_pretrained(\n",
    "    model_id,\n",
    "    num_labels=len(labels),\n",
    "    id2label={str(i): c for i, c in enumerate(labels)},\n",
    "    label2id={c: str(i) for i, c in enumerate(labels)}\n",
    ")\"\"\"\n",
    "\n",
    "#model = TFCvtForImageClassification.from_pretrained(\n",
    "#    model_id,\n",
    "#    )\n",
    "#model.classifier = tf.keras.layers.Dense(5)\n",
    "#model.num_labels = 5"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-23T11:44:54.587864400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "num_train_epochs = 5\n",
    "train_batch_size = 32\n",
    "eval_batch_size = 32\n",
    "learning_rate = 0.001\n",
    "weight_decay_rate = 0.01\n",
    "num_warmup_steps = 0\n",
    "output_dir = model_id.split(\"/\")[1]\n",
    "hub_model_id = f'{model_id.split(\"/\")[1]}-eyes'\n",
    "fp16 = True\n",
    "data_collator = DefaultDataCollator(return_tensors=\"tf\")\n",
    "\n",
    "# converting our train dataset to tf.data.Dataset\n",
    "tf_train_dataset = train_val_set[\"train\"].to_tf_dataset(\n",
    "    columns=['pixel_values'],\n",
    "    label_cols=[\"labels\"],\n",
    "    shuffle=True,\n",
    "    batch_size=train_batch_size,\n",
    "    collate_fn=data_collator)\n",
    "\n",
    "# converting our test dataset to tf.data.Dataset\n",
    "tf_eval_dataset = train_val_set[\"test\"].to_tf_dataset(\n",
    "    columns=['pixel_values'],\n",
    "    label_cols=[\"labels\"],\n",
    "    shuffle=True,\n",
    "    batch_size=eval_batch_size,\n",
    "    collate_fn=data_collator)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-23T11:44:54.588868300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for x, y in tf_train_dataset:\n",
    "    print(np.array(x[0]).min(), np.array(x[0]).max())\n",
    "    test = np.array(x[0])\n",
    "    print(test.shape)\n",
    "    plt.imshow(x[0])\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-23T11:44:54.589871900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"for image, label in tf_train_dataset.take(1):\n",
    "    img = image[0]\n",
    "    plt.imshow(image[0].numpy())\n",
    "    plt.show()\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-23T11:44:54.590875500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"class_names = [0,1,2,3,4]\n",
    "for i in range(len(class_names)):\n",
    "    filtered_ds = tf_train_dataset.filter(lambda x, l: tf.math.equal(l[0], i))\n",
    "    for image, label in filtered_ds.take(1):\n",
    "        ax = plt.subplot(3, 3, i+1)\n",
    "        plt.imshow(image[0].numpy().astype('uint8'))\n",
    "        plt.title(class_names[label.numpy()[0]])\n",
    "        plt.axis('off')\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-23T11:44:54.591878900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class model_per_epoch(keras.callbacks.Callback):\n",
    "    def __init__(self, model,filepath,save_best_only):\n",
    "        self.filepath=filepath\n",
    "        self.model=model\n",
    "        self.save_best_only=save_best_only\n",
    "        self.lowest_loss=np.inf\n",
    "        self.best_weights=self.model.get_weights()\n",
    "    def on_epoch_end(self,epoch, logs=None):\n",
    "        v_loss=logs.get('val_loss')\n",
    "        if v_loss< self.lowest_loss:\n",
    "            self.lowest_loss =v_loss\n",
    "            self.best_weights=self.model.get_weights()\n",
    "            self.best_epoch=epoch +1\n",
    "            self.model.set_weights(self.best_weights)\n",
    "            name= str(self.best_epoch) +'-' + str(self.lowest_loss)[:str(self.lowest_loss).rfind('.')+3] + '.h5'\n",
    "            file_id=os.path.join(self.filepath, name)\n",
    "            self.model.save_weights(file_id)\n",
    "        if self.save_best_only==False:\n",
    "            name= str(epoch) +'-' + str(v_loss)[:str(v_loss).rfind('.')+3] + '.h5'\n",
    "            file_id=os.path.join(self.filepath, name)\n",
    "            self.model.save(file_id)\n",
    "    def on_train_end(self, logs=None):\n",
    "        if self.save_best_only == True:\n",
    "            self.model.set_weights(self.best_weights)\n",
    "            name= str(self.best_epoch) +'-' + str(self.lowest_loss)[:str(self.lowest_loss).rfind('.')+3] + '.h5'\n",
    "            file_id=os.path.join(self.filepath, name)\n",
    "            self.model.save_weights(file_id)\n",
    "            print(' model is returned with best weights from epoch ', self.best_epoch)\n",
    "\n",
    "save_dir=r''"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-23T11:44:54.591878900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "metrics=[\n",
    "    tf.keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),\n",
    "]\n",
    "callbacks=[tf.keras.callbacks.EarlyStopping(patience=0),\n",
    "           model_per_epoch(model, save_dir, True)]\n",
    "model.compile(optimizer=\"adam\",\n",
    "              loss=\"sparse_categorical_crossentropy\",\n",
    "              metrics=\"accuracy\",)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-23T11:44:54.592881500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    tf_train_dataset.prefetch(20),\n",
    "    validation_data=tf_eval_dataset.prefetch(20),\n",
    "    callbacks=callbacks,\n",
    "    epochs=100,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2023-11-23T11:44:54.593885500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.save_weights(\"transformer_weights.h5\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-23T11:44:54.593885500Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
