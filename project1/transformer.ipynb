{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-11-23T11:47:32.173393800Z",
     "start_time": "2023-11-23T11:47:30.172041500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import keras.optimizers\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "from transformers import ViTImageProcessor\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import datasets\n",
    "from transformers import DefaultDataCollator\n",
    "from transformers import TFViTForImageClassification, create_optimizer\n",
    "from IPython.display import HTML\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support, accuracy_score\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "\n",
    "model_id = \"google/vit-base-patch16-224-in21k\"\n",
    "\n",
    "def create_image_folder_dataset(root_path):\n",
    "  \"\"\"creates `Dataset` from image folder structure\"\"\"\n",
    "\n",
    "  # get class names by folders names\n",
    "  _CLASS_NAMES= os.listdir(root_path)\n",
    "  # defines `datasets` features`\n",
    "  features=datasets.Features({\n",
    "                      \"img\": datasets.Image(),\n",
    "                      \"label\": datasets.features.ClassLabel(names=_CLASS_NAMES),\n",
    "                  })\n",
    "  # temp list holding datapoints for creation\n",
    "  img_data_files=[]\n",
    "  label_data_files=[]\n",
    "  # load images into list for creation\n",
    "  for img_class in os.listdir(root_path):\n",
    "    for img in os.listdir(os.path.join(root_path,img_class)):\n",
    "      path_=os.path.join(root_path,img_class,img)\n",
    "      img_data_files.append(path_)\n",
    "      label_data_files.append(img_class)\n",
    "  # create dataset\n",
    "  ds = datasets.Dataset.from_dict({\"img\":img_data_files,\"label\":label_data_files},features=features)\n",
    "  return ds"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-23T11:47:32.204474100Z",
     "start_time": "2023-11-23T11:47:32.175400900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "train_imgs = create_image_folder_dataset(\"train\")\n",
    "img_class_labels = train_imgs.features[\"label\"].names"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-23T11:47:32.221510600Z",
     "start_time": "2023-11-23T11:47:32.190426Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "feature_extractor = ViTImageProcessor.from_pretrained(model_id)\n",
    "\n",
    "# learn more about data augmentation here: https://www.tensorflow.org/tutorials/images/data_augmentation\n",
    "data_augmentation = keras.Sequential(\n",
    "    [\n",
    "        layers.Resizing(feature_extractor.size[\"height\"], feature_extractor.size[\"width\"]),\n",
    "        layers.Rescaling(1./255),\n",
    "        layers.RandomFlip(\"horizontal\"),\n",
    "        layers.RandomRotation(factor=0.2),\n",
    "        layers.RandomBrightness(factor=0.2),\n",
    "        layers.RandomContrast(factor=0.2),\n",
    "    ],\n",
    "    name=\"data_augmentation\",\n",
    ")\n",
    "# use keras image data augementation processing\n",
    "def augmentation(examples):\n",
    "    examples[\"pixel_values\"] =  [data_augmentation(image) for image in examples[\"img\"]]\n",
    "    process(examples)\n",
    "    return examples\n",
    "\n",
    "\n",
    "# basic processing (only resizing)\n",
    "def process(examples):\n",
    "    examples.update(feature_extractor(examples['img'], ))\n",
    "    #examples[\"pixel_values\"] = examples[\"pixel_values\"][0]\n",
    "    return examples\n",
    "\n",
    "# we are also renaming our label col to labels to use `.to_tf_dataset` later\n",
    "train_imgs = train_imgs.rename_column(\"label\", \"labels\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-23T11:47:32.658279200Z",
     "start_time": "2023-11-23T11:47:32.223517700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "test_size=.1\n",
    "train_val_set = train_imgs.train_test_split(test_size=test_size)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-23T11:47:32.674382600Z",
     "start_time": "2023-11-23T11:47:32.658279200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/408 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8a597fd28372414780a7b2e09c87fb71"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Saving the dataset (0/1 shards):   0%|          | 0/408 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0122cabfe151422a848c5b3113fb32a0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_val_set[\"test\"] = train_val_set[\"test\"].map(process, batch_size=16, batched=True)\n",
    "train_val_set[\"test\"].save_to_disk(\"test.hf\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-23T11:47:37.466390500Z",
     "start_time": "2023-11-23T11:47:32.674382600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/3672 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "dc82df490004496fa537cedc9af0de1a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/3672 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f2245c49ef1a469badd7d2b81fe2e6fb"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layers in a Sequential model should only have a single input tensor. Received: inputs=<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1024x765 at 0x15C08B704C0>. Consider rewriting this model with the Functional API.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling layer \"resizing\" \"                 f\"(type Resizing).\n\nAttempt to convert a value (<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1024x765 at 0x15C08B704C0>) with an unsupported type (<class 'PIL.JpegImagePlugin.JpegImageFile'>) to a Tensor.\n\nCall arguments received by layer \"resizing\" \"                 f\"(type Resizing):\n  â€¢ inputs=<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1024x765 at 0x15C08B704C0>",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[7], line 4\u001B[0m\n\u001B[0;32m      2\u001B[0m ds \u001B[38;5;241m=\u001B[39m train_val_set[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrain\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39mmap(process, batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m16\u001B[39m, batched\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[1;32m----> 4\u001B[0m     ds \u001B[38;5;241m=\u001B[39m datasets\u001B[38;5;241m.\u001B[39mconcatenate_datasets([ds, \u001B[43mtrain_val_set\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtrain\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmap\u001B[49m\u001B[43m(\u001B[49m\u001B[43maugmentation\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m16\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatched\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m])\n\u001B[0;32m      5\u001B[0m     ds\u001B[38;5;241m.\u001B[39msave_to_disk(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrain.hf\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\tf\\lib\\site-packages\\datasets\\arrow_dataset.py:591\u001B[0m, in \u001B[0;36mtransmit_tasks.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    589\u001B[0m     \u001B[38;5;28mself\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataset\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m=\u001B[39m kwargs\u001B[38;5;241m.\u001B[39mpop(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mself\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    590\u001B[0m \u001B[38;5;66;03m# apply actual function\u001B[39;00m\n\u001B[1;32m--> 591\u001B[0m out: Union[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataset\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDatasetDict\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    592\u001B[0m datasets: List[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataset\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(out\u001B[38;5;241m.\u001B[39mvalues()) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(out, \u001B[38;5;28mdict\u001B[39m) \u001B[38;5;28;01melse\u001B[39;00m [out]\n\u001B[0;32m    593\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m dataset \u001B[38;5;129;01min\u001B[39;00m datasets:\n\u001B[0;32m    594\u001B[0m     \u001B[38;5;66;03m# Remove task templates if a column mapping of the template is no longer valid\u001B[39;00m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\tf\\lib\\site-packages\\datasets\\arrow_dataset.py:556\u001B[0m, in \u001B[0;36mtransmit_format.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    549\u001B[0m self_format \u001B[38;5;241m=\u001B[39m {\n\u001B[0;32m    550\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtype\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_format_type,\n\u001B[0;32m    551\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mformat_kwargs\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_format_kwargs,\n\u001B[0;32m    552\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcolumns\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_format_columns,\n\u001B[0;32m    553\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moutput_all_columns\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_output_all_columns,\n\u001B[0;32m    554\u001B[0m }\n\u001B[0;32m    555\u001B[0m \u001B[38;5;66;03m# apply actual function\u001B[39;00m\n\u001B[1;32m--> 556\u001B[0m out: Union[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataset\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDatasetDict\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m func(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    557\u001B[0m datasets: List[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataset\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(out\u001B[38;5;241m.\u001B[39mvalues()) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(out, \u001B[38;5;28mdict\u001B[39m) \u001B[38;5;28;01melse\u001B[39;00m [out]\n\u001B[0;32m    558\u001B[0m \u001B[38;5;66;03m# re-apply format to the output\u001B[39;00m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\tf\\lib\\site-packages\\datasets\\arrow_dataset.py:3089\u001B[0m, in \u001B[0;36mDataset.map\u001B[1;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001B[0m\n\u001B[0;32m   3082\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m transformed_dataset \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m   3083\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m logging\u001B[38;5;241m.\u001B[39mtqdm(\n\u001B[0;32m   3084\u001B[0m         disable\u001B[38;5;241m=\u001B[39m\u001B[38;5;129;01mnot\u001B[39;00m logging\u001B[38;5;241m.\u001B[39mis_progress_bar_enabled(),\n\u001B[0;32m   3085\u001B[0m         unit\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m examples\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   3086\u001B[0m         total\u001B[38;5;241m=\u001B[39mpbar_total,\n\u001B[0;32m   3087\u001B[0m         desc\u001B[38;5;241m=\u001B[39mdesc \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMap\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   3088\u001B[0m     ) \u001B[38;5;28;01mas\u001B[39;00m pbar:\n\u001B[1;32m-> 3089\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m rank, done, content \u001B[38;5;129;01min\u001B[39;00m Dataset\u001B[38;5;241m.\u001B[39m_map_single(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mdataset_kwargs):\n\u001B[0;32m   3090\u001B[0m             \u001B[38;5;28;01mif\u001B[39;00m done:\n\u001B[0;32m   3091\u001B[0m                 shards_done \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\tf\\lib\\site-packages\\datasets\\arrow_dataset.py:3466\u001B[0m, in \u001B[0;36mDataset._map_single\u001B[1;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001B[0m\n\u001B[0;32m   3462\u001B[0m indices \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(\n\u001B[0;32m   3463\u001B[0m     \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m*\u001B[39m(\u001B[38;5;28mslice\u001B[39m(i, i \u001B[38;5;241m+\u001B[39m batch_size)\u001B[38;5;241m.\u001B[39mindices(shard\u001B[38;5;241m.\u001B[39mnum_rows)))\n\u001B[0;32m   3464\u001B[0m )  \u001B[38;5;66;03m# Something simpler?\u001B[39;00m\n\u001B[0;32m   3465\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m-> 3466\u001B[0m     batch \u001B[38;5;241m=\u001B[39m \u001B[43mapply_function_on_filtered_inputs\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   3467\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3468\u001B[0m \u001B[43m        \u001B[49m\u001B[43mindices\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3469\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcheck_same_num_examples\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mshard\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlist_indexes\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m>\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3470\u001B[0m \u001B[43m        \u001B[49m\u001B[43moffset\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moffset\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   3471\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   3472\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m NumExamplesMismatchError:\n\u001B[0;32m   3473\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m DatasetTransformationNotAllowedError(\n\u001B[0;32m   3474\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUsing `.map` in batched mode on a dataset with attached indexes is allowed only if it doesn\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt create or remove existing examples. You can first run `.drop_index() to remove your index and then re-add it.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   3475\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\tf\\lib\\site-packages\\datasets\\arrow_dataset.py:3345\u001B[0m, in \u001B[0;36mDataset._map_single.<locals>.apply_function_on_filtered_inputs\u001B[1;34m(pa_inputs, indices, check_same_num_examples, offset)\u001B[0m\n\u001B[0;32m   3343\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m with_rank:\n\u001B[0;32m   3344\u001B[0m     additional_args \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m (rank,)\n\u001B[1;32m-> 3345\u001B[0m processed_inputs \u001B[38;5;241m=\u001B[39m function(\u001B[38;5;241m*\u001B[39mfn_args, \u001B[38;5;241m*\u001B[39madditional_args, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mfn_kwargs)\n\u001B[0;32m   3346\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(processed_inputs, LazyDict):\n\u001B[0;32m   3347\u001B[0m     processed_inputs \u001B[38;5;241m=\u001B[39m {\n\u001B[0;32m   3348\u001B[0m         k: v \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m processed_inputs\u001B[38;5;241m.\u001B[39mdata\u001B[38;5;241m.\u001B[39mitems() \u001B[38;5;28;01mif\u001B[39;00m k \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m processed_inputs\u001B[38;5;241m.\u001B[39mkeys_to_format\n\u001B[0;32m   3349\u001B[0m     }\n",
      "Cell \u001B[1;32mIn[4], line 17\u001B[0m, in \u001B[0;36maugmentation\u001B[1;34m(examples)\u001B[0m\n\u001B[0;32m     16\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21maugmentation\u001B[39m(examples):\n\u001B[1;32m---> 17\u001B[0m     examples[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpixel_values\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m  [data_augmentation(image) \u001B[38;5;28;01mfor\u001B[39;00m image \u001B[38;5;129;01min\u001B[39;00m examples[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mimg\u001B[39m\u001B[38;5;124m\"\u001B[39m]]\n\u001B[0;32m     18\u001B[0m     process(examples)\n\u001B[0;32m     19\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m examples\n",
      "Cell \u001B[1;32mIn[4], line 17\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m     16\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21maugmentation\u001B[39m(examples):\n\u001B[1;32m---> 17\u001B[0m     examples[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpixel_values\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m  [\u001B[43mdata_augmentation\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimage\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m image \u001B[38;5;129;01min\u001B[39;00m examples[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mimg\u001B[39m\u001B[38;5;124m\"\u001B[39m]]\n\u001B[0;32m     18\u001B[0m     process(examples)\n\u001B[0;32m     19\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m examples\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\tf\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     67\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[0;32m     68\u001B[0m     \u001B[38;5;66;03m# To get the full stack trace, call:\u001B[39;00m\n\u001B[0;32m     69\u001B[0m     \u001B[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001B[39;00m\n\u001B[1;32m---> 70\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m     71\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m     72\u001B[0m     \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:102\u001B[0m, in \u001B[0;36mconvert_to_eager_tensor\u001B[1;34m(value, ctx, dtype)\u001B[0m\n\u001B[0;32m    100\u001B[0m     dtype \u001B[38;5;241m=\u001B[39m dtypes\u001B[38;5;241m.\u001B[39mas_dtype(dtype)\u001B[38;5;241m.\u001B[39mas_datatype_enum\n\u001B[0;32m    101\u001B[0m ctx\u001B[38;5;241m.\u001B[39mensure_initialized()\n\u001B[1;32m--> 102\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mops\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mEagerTensor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mctx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdevice_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mValueError\u001B[0m: Exception encountered when calling layer \"resizing\" \"                 f\"(type Resizing).\n\nAttempt to convert a value (<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1024x765 at 0x15C08B704C0>) with an unsupported type (<class 'PIL.JpegImagePlugin.JpegImageFile'>) to a Tensor.\n\nCall arguments received by layer \"resizing\" \"                 f\"(type Resizing):\n  â€¢ inputs=<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1024x765 at 0x15C08B704C0>"
     ]
    }
   ],
   "source": [
    "#processed_dataset_train = train_imgs.map(process)\n",
    "ds = train_val_set[\"train\"].map(process, batch_size=16, batched=True)\n",
    "while True:\n",
    "    ds = datasets.concatenate_datasets([ds, train_val_set[\"train\"].map(augmentation, batch_size=16, batched=True)])\n",
    "    ds.save_to_disk(\"train.hf\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-23T11:48:19.543918600Z",
     "start_time": "2023-11-23T11:47:37.467394600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-23T11:48:19.548936200Z",
     "start_time": "2023-11-23T11:48:19.545925100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_val_set = {}\n",
    "train_val_set[\"train\"] = datasets.load_from_disk(\"train.hf\")\n",
    "train_val_set[\"test\"] = datasets.load_from_disk(\"test.hf\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-23T11:48:19.546928800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "id2label = {str(i): label for i, label in enumerate(img_class_labels)}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "num_train_epochs = 5\n",
    "train_batch_size = 32\n",
    "eval_batch_size = 32\n",
    "learning_rate = 3e-5\n",
    "weight_decay_rate=0.01\n",
    "num_warmup_steps=0\n",
    "output_dir=model_id.split(\"/\")[1]\n",
    "hub_model_id = f'{model_id.split(\"/\")[1]}-eyes'\n",
    "fp16=True\n",
    "\n",
    "# Train in mixed-precision float16\n",
    "# Comment this line out if you're using a GPU that will not benefit from this\n",
    "if fp16:\n",
    "  tf.keras.mixed_precision.set_global_policy(\"mixed_float16\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-23T11:48:19.547932200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Data collator that will dynamically pad the inputs received, as well as the labels.\n",
    "data_collator = DefaultDataCollator(return_tensors=\"tf\")\n",
    "\n",
    "# converting our train dataset to tf.data.Dataset\n",
    "tf_train_dataset = train_val_set[\"train\"].to_tf_dataset(\n",
    "   columns=['pixel_values'],\n",
    "   label_cols=[\"labels\"],\n",
    "   shuffle=True,\n",
    "   batch_size=train_batch_size,\n",
    "   collate_fn=data_collator)\n",
    "\n",
    "# converting our test dataset to tf.data.Dataset\n",
    "tf_eval_dataset = train_val_set[\"test\"].to_tf_dataset(\n",
    "   columns=['pixel_values'],\n",
    "   label_cols=[\"labels\"],\n",
    "   shuffle=True,\n",
    "   batch_size=eval_batch_size,\n",
    "   collate_fn=data_collator)\n",
    "processed_dataset = None"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-23T11:48:19.548936200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tf_train_dataset"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-23T11:48:19.549939700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# create optimizer wight weigh decay\n",
    "num_train_steps = len(tf_train_dataset) * num_train_epochs\n",
    "optimizer, lr_schedule = create_optimizer(\n",
    "    init_lr=learning_rate,\n",
    "    num_train_steps=num_train_steps,\n",
    "    weight_decay_rate=weight_decay_rate,\n",
    "    num_warmup_steps=num_warmup_steps,\n",
    ")\n",
    "\n",
    "# load pre-trained ViT model\n",
    "model = TFViTForImageClassification.from_pretrained(\n",
    "    model_id,\n",
    "    num_labels=len(img_class_labels),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")\n",
    "\n",
    "# define loss\n",
    "loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "# define metrics\n",
    "metrics=[\n",
    "    tf.keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),\n",
    "]\n",
    "callbacks=[tf.keras.callbacks.EarlyStopping(patience=0)]\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss=loss,\n",
    "              metrics=metrics,)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-23T11:48:19.550942600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    tf_train_dataset.prefetch(1),\n",
    "    validation_data=tf_eval_dataset.prefetch(1),\n",
    "    callbacks=callbacks,\n",
    "    epochs=100,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-23T11:48:19.551946600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    tf_train_dataset.prefetch(3),\n",
    "    validation_data=tf_eval_dataset.prefetch(3),\n",
    "    callbacks=callbacks,\n",
    "    epochs=100,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-23T11:48:19.552950200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_imgs = create_image_folder_dataset(\"test\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-23T11:48:19.553953500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_imgs = test_imgs.rename_column(\"label\", \"labels\")\n",
    "processed_dataset_test = test_imgs.map(process, batched=True, batch_size=32)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-23T11:48:19.553953500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tf_test_dataset = processed_dataset_test.to_tf_dataset(\n",
    "   columns=['pixel_values'],\n",
    "   label_cols=[\"labels\"],\n",
    "   shuffle=False,\n",
    "   batch_size=eval_batch_size,\n",
    "   collate_fn=data_collator)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-23T11:48:19.554956600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_pred = model.predict(tf_test_dataset.prefetch(2))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-23T11:48:19.555960600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_pred_norm = []\n",
    "for vector in y_pred[\"logits\"]:\n",
    "    y_pred_norm.append(np.argmax(vector))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-23T11:48:19.556964Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "def evaluate(y_t, y_p):\n",
    "    cm = confusion_matrix(y_t, y_p)\n",
    "    ax = sn.heatmap(cm, annot=True, fmt='g') # font size\n",
    "    ax.set(xlabel=\"Predicted Number\", ylabel=\"True Number\")\n",
    "    plt.show()\n",
    "\n",
    "    prfs = pd.DataFrame()\n",
    "    precision, recall, fscore, support = precision_recall_fscore_support(y_t, y_p)\n",
    "    prfs[\"Digit\"] = range(0, 5)\n",
    "    prfs[\"Precision\"] = precision\n",
    "    prfs[\"Recall\"] = recall\n",
    "    prfs[\"F1-score\"] = fscore\n",
    "    prfs[\"Occurances\"] = support\n",
    "    print(\"The accuracy of the model is \" + str(round(100 * accuracy_score(y_t, y_p),2)) + \"%.\")\n",
    "    return prfs.to_html(index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-23T11:48:19.556964Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "HTML(evaluate(list(processed_dataset_test[\"labels\"]), y_pred_norm))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-23T11:48:19.557967600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.save_weights(\"augmented.hdf5\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-23T11:48:19.558971Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
