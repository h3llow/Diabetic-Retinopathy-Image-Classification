{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-12-15T13:01:23.525445600Z",
     "start_time": "2023-12-15T13:01:23.517729Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "from torchvision import models\n",
    "import torchvision.transforms as T\n",
    "\n",
    "# Load the pre-trained keypoint detection model\n",
    "model = models.detection.keypointrcnn_resnet50_fpn(pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "# Open webcam\n",
    "cap = cv2.VideoCapture(0)  # 0 corresponds to the default camera (usually the built-in webcam)\n",
    "\n",
    "# Define connections between keypoints\n",
    "keypoint_connections = [\n",
    "    (0, 1), (0, 2),  # Connect Nose to Left and Right Eyes\n",
    "    (1, 3), (2, 4),  # Connect Left and Right Eyes to Left and Right Ears\n",
    "    (5, 6), (5, 7), (7, 9), (6, 8), (8, 10),  # Connect Shoulders, Elbows, and Wrists\n",
    "    (5, 11), (6, 12),\n",
    "    (11, 12), (11, 13), (13, 15), (12, 14), (14, 16)  # Connect Hips, Knees, and Ankles\n",
    "]\n",
    "\n",
    "# Indices corresponding to the feet keypoints\n",
    "left_foot_index = 15\n",
    "right_foot_index = 16\n",
    "\n",
    "# Define region of interest (ROI) for legs\n",
    "min_y_leg_roi = 300  # Adjust this value based on your frame dimensions\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Convert BGR image to RGB\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Apply the necessary transformations\n",
    "    transform = T.Compose([T.ToTensor()])\n",
    "    input_tensor = transform(rgb_frame).unsqueeze(0)\n",
    "\n",
    "    # Make prediction\n",
    "    with torch.no_grad():\n",
    "        predictions = model(input_tensor)\n",
    "\n",
    "    # Check if predictions contain elements and 'keypoints' is present\n",
    "    if predictions and 'keypoints' in predictions[0]:\n",
    "        # Extract keypoints from the prediction\n",
    "        if predictions[0]['keypoints'].shape[0] > 0:\n",
    "            keypoints = predictions[0]['keypoints'][0].numpy()\n",
    "    \n",
    "            # Draw keypoints on the frame\n",
    "            for kp in keypoints:\n",
    "                x, y, prob = map(int, kp)\n",
    "                if prob > 0.5:  # Draw keypoints with confidence greater than 0.5\n",
    "                    cv2.circle(frame, (x, y), 5, (0, 255, 0), -1)\n",
    "    \n",
    "            # Draw lines connecting keypoints\n",
    "            for connection in keypoint_connections:\n",
    "                start_point = connection[0]\n",
    "                end_point = connection[1]\n",
    "    \n",
    "                # Check if both keypoints in a connection have sufficient confidence\n",
    "                if keypoints[start_point][2] > 0.5 and keypoints[end_point][2] > 0.5:\n",
    "                    start_coord = tuple(map(int, keypoints[start_point][:2]))\n",
    "                    end_coord = tuple(map(int, keypoints[end_point][:2]))\n",
    "    \n",
    "                    cv2.line(frame, start_coord, end_coord, (0, 255, 0), 2)\n",
    "    \n",
    "            # Check if feet keypoints are visible and within the leg ROI before drawing them\n",
    "            if keypoints[left_foot_index][2] > 0.5 and keypoints[left_foot_index][1] > min_y_leg_roi:\n",
    "                cv2.circle(frame, tuple(map(int, keypoints[left_foot_index][:2])), 5, (0, 255, 0), -1)\n",
    "    \n",
    "            if keypoints[right_foot_index][2] > 0.5 and keypoints[right_foot_index][1] > min_y_leg_roi:\n",
    "                cv2.circle(frame, tuple(map(int, keypoints[right_foot_index][:2])), 5, (0, 255, 0), -1)\n",
    "\n",
    "    # Display the frame\n",
    "    cv2.imshow('Pose Estimation', frame)\n",
    "\n",
    "    # Break the loop when 'q' key is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the webcam and close the window\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-15T13:39:24.462610800Z",
     "start_time": "2023-12-15T13:38:16.606660300Z"
    }
   },
   "id": "e39fd034d36c0a3c"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "config.json:   0%|          | 0.00/920 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0a37db6e5cf843ca87e2619d6b658775"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mettn\\miniconda3\\envs\\pt\\Lib\\site-packages\\huggingface_hub\\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\mettn\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "text/plain": "pytorch_model.bin:   0%|          | 0.00/245M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d45979e667a94773918769135f55352b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "preprocessor_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "89e21e9199494b59b46a7a634b815d1f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "checkpoint = \"vinvino02/glpn-nyu\"\n",
    "depth_estimator = pipeline(\"depth-estimation\", model=checkpoint)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-15T15:04:04.147704700Z",
     "start_time": "2023-12-15T15:03:38.474998600Z"
    }
   },
   "id": "9b6948e1a1bc1544"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Device :  cuda:0\n",
      "(0,)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import time\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "\n",
    "from utils.datasets import letterbox\n",
    "from utils.general import non_max_suppression_kpt\n",
    "from utils.plots import output_to_keypoint, plot_skeleton_kpts\n",
    "\n",
    "def pose_video(frame):\n",
    "    mapped_img = frame.copy()\n",
    "    # Letterbox resizing.\n",
    "    img = letterbox(frame, input_size, stride=64, auto=True)[0]\n",
    "    #print(img.shape)\n",
    "    img_ = img.copy()\n",
    "    # Convert the array to 4D.\n",
    "    img = transforms.ToTensor()(img)\n",
    "    # Convert the array to Tensor.\n",
    "    img = torch.tensor(np.array([img.numpy()]))\n",
    "    # Load the image into the computation device.\n",
    "    img = img.to(device)\n",
    "    \n",
    "    depth = depth_estimator(Image.fromarray(frame))[\"depth\"]\n",
    "    # Gradients are stored during training, not required while inference.\n",
    "    with torch.no_grad():\n",
    "        t1 = time.time()\n",
    "        output, _ = model(img)\n",
    "        \n",
    "        \n",
    "        t2 = time.time()\n",
    "        fps = 1/(t2 - t1)\n",
    "        output = non_max_suppression_kpt(output, \n",
    "                                         0.25,    # Conf. Threshold.\n",
    "                                         0.65,    # IoU Threshold.\n",
    "                                         nc=1,   # Number of classes.\n",
    "                                         nkpt=17, # Number of keypoints.\n",
    "                                         kpt_label=True)\n",
    "        \n",
    "        output = output_to_keypoint(output)\n",
    "        print(output.shape)\n",
    "    # Change format [b, c, h, w] to [h, w, c] for displaying the image.\n",
    "    nimg = img[0].permute(1, 2, 0) * 255\n",
    "    nimg = nimg.cpu().numpy().astype(np.uint8)\n",
    "    nimg = cv2.cvtColor(nimg, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    for idx in range(output.shape[0]):\n",
    "        plot_skeleton_kpts(nimg, mapped_img, input_size, output[idx, 7:].T, 3)\n",
    "        \n",
    "    return nimg, fps\n",
    "# Change forward pass input size.\n",
    "input_size = 960\n",
    "\n",
    "# Select the device based on hardware configs.\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print('Selected Device : ', device)\n",
    "\n",
    "# Load keypoint detection model.\n",
    "weights = torch.load('yolov7-w6-pose.pt', map_location=device)\n",
    "model = weights['model']\n",
    "# Load the model in evaluation mode.\n",
    "_ = model.float().eval()\n",
    "# Load the model to computation device [cpu/gpu/tpu]\n",
    "model.to(device)\n",
    "\n",
    "# Webcam capture\n",
    "cap = cv2.VideoCapture(0)  # 0 corresponds to the default camera (usually the built-in webcam)\n",
    "\n",
    "# May need to change the w, h as letterbox function reshapes the image.\n",
    "w = 1920#int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "h = 1080#int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "# Video writer initialization\n",
    "out = cv2.VideoWriter('pose_outputs/webcam_output.mp4',\n",
    "                      cv2.VideoWriter_fourcc(*'mp4v'),\n",
    "                      30, (w, h))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        if not ret:\n",
    "            print('Unable to read frame. Exiting ..')\n",
    "            break\n",
    "\n",
    "        mapped_img = frame.copy()\n",
    "        # Letterbox resizing.\n",
    "        img = letterbox(frame, input_size, stride=64, auto=True)[0]\n",
    "        #print(img.shape)\n",
    "        img_ = img.copy()\n",
    "        # Convert the array to 4D.\n",
    "        img = transforms.ToTensor()(img)\n",
    "        # Convert the array to Tensor.\n",
    "        img = torch.tensor(np.array([img.numpy()]))\n",
    "        # Load the image into the computation device.\n",
    "        img = img.to(device)\n",
    "        pil_img = Image.fromarray(frame)\n",
    "        depth = depth_estimator(pil_img)[\"predicted_depth\"]\n",
    "        # Gradients are stored during training, not required while inference.\n",
    "        with torch.no_grad():\n",
    "            t1 = time.time()\n",
    "            output, _ = model(img)\n",
    "            \n",
    "            \n",
    "            t2 = time.time()\n",
    "            fps = 1/(t2 - t1)\n",
    "            output = non_max_suppression_kpt(output, \n",
    "                                             0.25,    # Conf. Threshold.\n",
    "                                             0.65,    # IoU Threshold.\n",
    "                                             nc=1,   # Number of classes.\n",
    "                                             nkpt=17, # Number of keypoints.\n",
    "                                             kpt_label=True)\n",
    "            \n",
    "            output = output_to_keypoint(output)\n",
    "            print(output.shape)\n",
    "        # Change format [b, c, h, w] to [h, w, c] for displaying the image.\n",
    "        nimg = img[0].permute(1, 2, 0) * 255\n",
    "        nimg = nimg.cpu().numpy().astype(np.uint8)\n",
    "        nimg = cv2.cvtColor(nimg, cv2.COLOR_RGB2BGR)\n",
    "    \n",
    "        for idx in range(output.shape[0]):\n",
    "            plot_skeleton_kpts(nimg, mapped_img, input_size, output[idx, 7:].T, 3)\n",
    "        cv2.putText(nimg, 'FPS : {:.2f}'.format(fps), (200, 40), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2,\n",
    "                    cv2.LINE_AA)\n",
    "        cv2.putText(nimg, 'YOLOv7', (20, 40), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "        cv2.imshow('Output', nimg[..., ::-1])\n",
    "        out.write(nimg[..., ::-1])\n",
    "        key = cv2.waitKey(1)\n",
    "        if key == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-15T15:16:48.401153300Z",
     "start_time": "2023-12-15T15:16:22.671814500Z"
    }
   },
   "id": "d5ee8b2ab7fe4b70"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as T\n",
    "transform = T.ToPILImage()\n",
    "transform(depth).show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-15T15:20:45.398956400Z",
     "start_time": "2023-12-15T15:20:42.058063900Z"
    }
   },
   "id": "8b364a402ae66e2c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
