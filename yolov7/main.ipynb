{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-12-16T13:34:34.919938900Z",
     "start_time": "2023-12-16T13:34:34.918426Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-16T13:03:50.936089400Z",
     "start_time": "2023-12-16T13:03:49.222519Z"
    }
   },
   "id": "9b6948e1a1bc1544"
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Device :  cuda:0\n",
      "(0,)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(2, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(2, 58)\n",
      "(2, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(2, 58)\n",
      "(2, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(2, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(2, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n",
      "(1, 58)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[52], line 72\u001B[0m\n\u001B[0;32m     70\u001B[0m     t2 \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n\u001B[0;32m     71\u001B[0m     fps \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\u001B[38;5;241m/\u001B[39m(t2 \u001B[38;5;241m-\u001B[39m t1)\n\u001B[1;32m---> 72\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[43mnon_max_suppression_kpt\u001B[49m\u001B[43m(\u001B[49m\u001B[43moutput\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\n\u001B[0;32m     73\u001B[0m \u001B[43m                                     \u001B[49m\u001B[38;5;241;43m0.25\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m    \u001B[49m\u001B[38;5;66;43;03m# Conf. Threshold.\u001B[39;49;00m\n\u001B[0;32m     74\u001B[0m \u001B[43m                                     \u001B[49m\u001B[38;5;241;43m0.65\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m    \u001B[49m\u001B[38;5;66;43;03m# IoU Threshold.\u001B[39;49;00m\n\u001B[0;32m     75\u001B[0m \u001B[43m                                     \u001B[49m\u001B[43mnc\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m   \u001B[49m\u001B[38;5;66;43;03m# Number of classes.\u001B[39;49;00m\n\u001B[0;32m     76\u001B[0m \u001B[43m                                     \u001B[49m\u001B[43mnkpt\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m17\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;66;43;03m# Number of keypoints.\u001B[39;49;00m\n\u001B[0;32m     77\u001B[0m \u001B[43m                                     \u001B[49m\u001B[43mkpt_label\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m     78\u001B[0m     output2key \u001B[38;5;241m=\u001B[39m output_to_keypoint(output)\n\u001B[0;32m     81\u001B[0m points \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mzeros((output2key\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m], \u001B[38;5;241m17\u001B[39m, \u001B[38;5;241m3\u001B[39m))\n",
      "File \u001B[1;32m~\\Documents\\repos\\aaai_pose_estimation\\yolov7\\utils\\general.py:781\u001B[0m, in \u001B[0;36mnon_max_suppression_kpt\u001B[1;34m(prediction, conf_thres, iou_thres, classes, agnostic, multi_label, labels, kpt_label, nc, nkpt)\u001B[0m\n\u001B[0;32m    779\u001B[0m c \u001B[38;5;241m=\u001B[39m x[:, \u001B[38;5;241m5\u001B[39m:\u001B[38;5;241m6\u001B[39m] \u001B[38;5;241m*\u001B[39m (\u001B[38;5;241m0\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m agnostic \u001B[38;5;28;01melse\u001B[39;00m max_wh)  \u001B[38;5;66;03m# classes\u001B[39;00m\n\u001B[0;32m    780\u001B[0m boxes, scores \u001B[38;5;241m=\u001B[39m x[:, :\u001B[38;5;241m4\u001B[39m] \u001B[38;5;241m+\u001B[39m c, x[:, \u001B[38;5;241m4\u001B[39m]  \u001B[38;5;66;03m# boxes (offset by class), scores\u001B[39;00m\n\u001B[1;32m--> 781\u001B[0m i \u001B[38;5;241m=\u001B[39m \u001B[43mtorchvision\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mops\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnms\u001B[49m\u001B[43m(\u001B[49m\u001B[43mboxes\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mscores\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43miou_thres\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# NMS\u001B[39;00m\n\u001B[0;32m    782\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m i\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m>\u001B[39m max_det:  \u001B[38;5;66;03m# limit detections\u001B[39;00m\n\u001B[0;32m    783\u001B[0m     i \u001B[38;5;241m=\u001B[39m i[:max_det]\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\pt\\Lib\\site-packages\\torchvision\\ops\\boxes.py:13\u001B[0m, in \u001B[0;36mnms\u001B[1;34m(boxes, scores, iou_threshold)\u001B[0m\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_box_convert\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m _box_cxcywh_to_xyxy, _box_xywh_to_xyxy, _box_xyxy_to_cxcywh, _box_xyxy_to_xywh\n\u001B[0;32m     10\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_utils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m _upcast\n\u001B[1;32m---> 13\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mnms\u001B[39m(boxes: Tensor, scores: Tensor, iou_threshold: \u001B[38;5;28mfloat\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m     14\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m     15\u001B[0m \u001B[38;5;124;03m    Performs non-maximum suppression (NMS) on the boxes according\u001B[39;00m\n\u001B[0;32m     16\u001B[0m \u001B[38;5;124;03m    to their intersection-over-union (IoU).\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     36\u001B[0m \u001B[38;5;124;03m        by NMS, sorted in decreasing order of scores\u001B[39;00m\n\u001B[0;32m     37\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m     38\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mjit\u001B[38;5;241m.\u001B[39mis_scripting() \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mjit\u001B[38;5;241m.\u001B[39mis_tracing():\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import socket\n",
    "import time\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "import pickle as pkl\n",
    "\n",
    "from utils.datasets import letterbox\n",
    "from utils.general import non_max_suppression_kpt\n",
    "from utils.plots import output_to_keypoint, plot_skeleton_kpts\n",
    "\n",
    "# Change forward pass input size.\n",
    "input_size = 960\n",
    "\n",
    "# Select the device based on hardware configs.\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print('Selected Device : ', device)\n",
    "\n",
    "# Load keypoint detection model.\n",
    "weights = torch.load('yolov7-w6-pose.pt', map_location=device)\n",
    "model = weights['model']\n",
    "# Load the model in evaluation mode.\n",
    "_ = model.float().eval()\n",
    "# Load the model to computation device [cpu/gpu/tpu]\n",
    "model.to(device)\n",
    "\n",
    "# Webcam capture\n",
    "cap = cv2.VideoCapture(0)  # 0 corresponds to the default camera (usually the built-in webcam)\n",
    "\n",
    "# May need to change the w, h as letterbox function reshapes the image.\n",
    "w = 1920#int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "h = 1080#int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "# Video writer initialization\n",
    "out = cv2.VideoWriter('pose_outputs/webcam_output.mp4',\n",
    "                      cv2.VideoWriter_fourcc(*'mp4v'),\n",
    "                      30, (w, h))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "   while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        if not ret:\n",
    "            print('Unable to read frame. Exiting ..')\n",
    "            break\n",
    "\n",
    "        mapped_img = frame.copy()\n",
    "        # Letterbox resizing.\n",
    "        img = letterbox(frame, input_size, stride=64, auto=True)[0]\n",
    "        #print(img.shape)\n",
    "        img_ = img.copy()\n",
    "        # Convert the array to 4D.\n",
    "        img = transforms.ToTensor()(img)\n",
    "        # Convert the array to Tensor.\n",
    "        img = torch.tensor(np.array([img.numpy()]))\n",
    "        # Load the image into the computation device.\n",
    "        img = img.to(device)\n",
    "        pil_img = Image.fromarray(frame)\n",
    "        # Gradients are stored during training, not required while inference.\n",
    "        with torch.no_grad():\n",
    "            t1 = time.time()\n",
    "            output, _ = model(img)\n",
    "            \n",
    "            \n",
    "            t2 = time.time()\n",
    "            fps = 1/(t2 - t1)\n",
    "            output = non_max_suppression_kpt(output, \n",
    "                                             0.25,    # Conf. Threshold.\n",
    "                                             0.65,    # IoU Threshold.\n",
    "                                             nc=1,   # Number of classes.\n",
    "                                             nkpt=17, # Number of keypoints.\n",
    "                                             kpt_label=True)\n",
    "            output2key = output_to_keypoint(output)\n",
    "            \n",
    "            \n",
    "        points = np.zeros((output2key.shape[0], 17, 3))\n",
    "        \n",
    "        for entity in range(output2key.shape[0]):\n",
    "            keypoints = output2key[entity, 7:].T\n",
    "            for kpt_idx in range(17):  # Assuming you have 17 keypoints\n",
    "                idx = kpt_idx * 3  # Each keypoint has x, y, and confidence values\n",
    "                x, y, conf = keypoints[idx:idx+3]\n",
    "                if conf > 0.6:\n",
    "                    points[entity, kpt_idx] = [x/10,  y/10]\n",
    "                else:\n",
    "                    points[entity, kpt_idx] = [-1, -1, -1]\n",
    "        print(output2key.shape)\n",
    "        # Change format [b, c, h, w] to [h, w, c] for displaying the image.\n",
    "        nimg = img[0].permute(1, 2, 0) * 255\n",
    "        nimg = nimg.cpu().numpy().astype(np.uint8)\n",
    "        nimg = cv2.cvtColor(nimg, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        for idx in range(output2key.shape[0]):\n",
    "            plot_skeleton_kpts(nimg, mapped_img, input_size, output2key[idx, 7:].T, 3)\n",
    "        cv2.putText(nimg, 'FPS : {:.2f}'.format(fps), (200, 40), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2,\n",
    "                    cv2.LINE_AA)\n",
    "        cv2.putText(nimg, 'YOLOv7', (20, 40), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "        cv2.imshow('Output', nimg[..., ::-1])\n",
    "        out.write(nimg[..., ::-1])\n",
    "        key = cv2.waitKey(1)\n",
    "        if key == ord('q'):\n",
    "            break\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-16T16:26:11.283197500Z",
     "start_time": "2023-12-16T16:25:14.996007Z"
    }
   },
   "id": "d5ee8b2ab7fe4b70"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Device :  cuda:0\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001B[1;31mSystemExit\u001B[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mettn\\miniconda3\\envs\\pt\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3556: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import socket\n",
    "import time\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "import pickle as pkl\n",
    "\n",
    "from utils.datasets import letterbox\n",
    "from utils.general import non_max_suppression_kpt\n",
    "from utils.plots import output_to_keypoint, plot_skeleton_kpts\n",
    "\n",
    "# Change forward pass input size.\n",
    "input_size = 960\n",
    "\n",
    "# Select the device based on hardware configs.\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print('Selected Device : ', device)\n",
    "\n",
    "# Load keypoint detection model.\n",
    "weights = torch.load('yolov7-w6-pose.pt', map_location=device)\n",
    "model = weights['model']\n",
    "# Load the model in evaluation mode.\n",
    "_ = model.float().eval()\n",
    "# Load the model to computation device [cpu/gpu/tpu]\n",
    "model.to(device)\n",
    "\n",
    "# Webcam capture\n",
    "cap = cv2.VideoCapture(0)  # 0 corresponds to the default camera (usually the built-in webcam)\n",
    "\n",
    "# May need to change the w, h as letterbox function reshapes the image.\n",
    "w = 1920#int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "h = 1080#int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "# Video writer initialization\n",
    "out = cv2.VideoWriter('pose_outputs/webcam_output.mp4',\n",
    "                      cv2.VideoWriter_fourcc(*'mp4v'),\n",
    "                      30, (w, h))\n",
    "\n",
    "import pygame\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "\n",
    "# Initialize Pygame\n",
    "pygame.init()\n",
    "\n",
    "# Constants\n",
    "WIDTH, HEIGHT = 960, 768\n",
    "INITIAL_BALL_SPEED = 20\n",
    "BALL_SPEED = INITIAL_BALL_SPEED\n",
    "BALL_ACCELERATION = 1.001\n",
    "PADDLE_SPEED = 8\n",
    "WHITE = (255, 255, 255)\n",
    "FPS = 60\n",
    "\n",
    "# Initialize screen\n",
    "screen = pygame.display.set_mode((WIDTH, HEIGHT))\n",
    "pygame.display.set_caption(\"Pong Game\")\n",
    "\n",
    "# Initialize clock\n",
    "clock = pygame.time.Clock()\n",
    "\n",
    "# Initialize fonts\n",
    "font = pygame.font.Font(None, 36)\n",
    "\n",
    "# Initialize variables\n",
    "ball = pygame.Rect(WIDTH // 2 - 15, HEIGHT // 2 - 15, 30, 30)\n",
    "ball_speed = [random.choice([-1, 1]) * INITIAL_BALL_SPEED, random.uniform(-1, 1) * INITIAL_BALL_SPEED]\n",
    "\n",
    "player1 = pygame.Rect(20, HEIGHT // 2 - 50, 10, 100)\n",
    "player2 = pygame.Rect(WIDTH - 30, HEIGHT // 2 - 50, 10, 100)\n",
    "\n",
    "score_player1 = 0\n",
    "score_player2 = 0\n",
    "\n",
    "ball_scored = False  # Flag to prevent multiple scoring for the same event\n",
    "game_over = False  # Flag to indicate whether the game is over\n",
    "\n",
    "def reset_game():\n",
    "    global ball, ball_speed, ball_scored, game_over\n",
    "    ball = pygame.Rect(WIDTH // 2 - 15, HEIGHT // 2 - 15, 30, 30)\n",
    "    ball_speed = [random.choice([-1, 1]) * INITIAL_BALL_SPEED, random.uniform(-1, 1) * INITIAL_BALL_SPEED]\n",
    "    ball_scored = False\n",
    "\n",
    "def draw_text(text, x, y):\n",
    "    text_surface = font.render(text, True, WHITE)\n",
    "    text_rect = text_surface.get_rect(center=(x, y))\n",
    "    screen.blit(text_surface, text_rect)\n",
    "\n",
    "while cap.isOpened():\n",
    "    BALL_SPEED *= BALL_ACCELERATION\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        print('Unable to read frame. Exiting ..')\n",
    "        break\n",
    "\n",
    "    mapped_img = frame.copy()\n",
    "    # Letterbox resizing.\n",
    "    img = letterbox(frame, input_size, stride=64, auto=True)[0]\n",
    "    #print(img.shape)\n",
    "    img_ = img.copy()\n",
    "    # Convert the array to 4D.\n",
    "    img = transforms.ToTensor()(img)\n",
    "    # Convert the array to Tensor.\n",
    "    img = torch.tensor(np.array([img.numpy()]))\n",
    "    # Load the image into the computation device.\n",
    "    img = img.to(device)\n",
    "    pil_img = Image.fromarray(frame)\n",
    "    # Gradients are stored during training, not required while inference.\n",
    "    with torch.no_grad():\n",
    "        t1 = time.time()\n",
    "        output, _ = model(img)\n",
    "        \n",
    "        \n",
    "        t2 = time.time()\n",
    "        fps = 1/(t2 - t1)\n",
    "        output = non_max_suppression_kpt(output, \n",
    "                                         0.25,    # Conf. Threshold.\n",
    "                                         0.65,    # IoU Threshold.\n",
    "                                         nc=1,   # Number of classes.\n",
    "                                         nkpt=17, # Number of keypoints.\n",
    "                                         kpt_label=True)\n",
    "        output2key = output_to_keypoint(output)\n",
    "        \n",
    "        \n",
    "    points = np.zeros((output2key.shape[0], 17, 2))\n",
    "    \n",
    "    for entity in range(output2key.shape[0]):\n",
    "        keypoints = output2key[entity, 7:].T\n",
    "        for kpt_idx in range(17):  # Assuming you have 17 keypoints\n",
    "            idx = kpt_idx * 3  # Each keypoint has x, y, and confidence values\n",
    "            x, y, conf = keypoints[idx:idx+3]\n",
    "            y = max(0, min(1, (y-150)/568))\n",
    "            x = max(0, min(1, (x-50)/960))\n",
    "            if conf > 0.6:\n",
    "                points[entity, kpt_idx] = [x,  y]\n",
    "            else:\n",
    "                points[entity, kpt_idx] = [-1, -1]\n",
    "    # Change format [b, c, h, w] to [h, w, c] for displaying the image.\n",
    "    nimg = img[0].permute(1, 2, 0) * 255\n",
    "    nimg = nimg.cpu().numpy().astype(np.uint8)\n",
    "    nimg = cv2.cvtColor(nimg, cv2.COLOR_RGB2BGR)\n",
    "    \n",
    "    for idx in range(output2key.shape[0]):\n",
    "        plot_skeleton_kpts(nimg, mapped_img, input_size, output2key[idx, 7:].T, 3)\n",
    "    cv2.putText(nimg, 'FPS : {:.2f}'.format(fps), (200, 40), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2,\n",
    "                cv2.LINE_AA)\n",
    "    cv2.putText(nimg, 'YOLOv7', (20, 40), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "    cv2.imshow('Output', nimg[..., ::-1])\n",
    "    out.write(nimg[..., ::-1])\n",
    "    key = cv2.waitKey(1)\n",
    "    if key == ord('q'):\n",
    "        break\n",
    "            \n",
    "            \n",
    "    for event in pygame.event.get():\n",
    "        if event.type == pygame.QUIT:\n",
    "            pygame.quit()\n",
    "            sys.exit()\n",
    "\n",
    "    keys = pygame.key.get_pressed()\n",
    "\n",
    "    # Check if the game is over\n",
    "    if not game_over:\n",
    "        # Update player1 position\n",
    "        #player1.y += (keys[pygame.K_s] - keys[pygame.K_w]) * PADDLE_SPEED\n",
    "        if len(points) > 0:\n",
    "            if points[0, 9, 1] != -1:\n",
    "                player1.y = points[0, 9, 1] * HEIGHT\n",
    "        else:\n",
    "            player1.y = 0\n",
    "        player1.y = max(0, min(player1.y, HEIGHT - player1.height))\n",
    "\n",
    "        # Update player2 position\n",
    "        #player2.y += (keys[pygame.K_s] - keys[pygame.K_w]) * PADDLE_SPEED\n",
    "        if len(points) > 0:\n",
    "            if points[0, 10, 1] != -1:\n",
    "                player2.y = points[0, 10, 1] * HEIGHT\n",
    "        else:\n",
    "            player2.y = 0\n",
    "        player2.y = max(0, min(player2.y, HEIGHT - player2.height))\n",
    "\n",
    "        # Ball movement\n",
    "        ball.x += ball_speed[0]\n",
    "        ball.y += ball_speed[1]\n",
    "\n",
    "        # Ball collisions\n",
    "        if ball.colliderect(player1) or ball.colliderect(player2):\n",
    "            ball_speed[0] = -ball_speed[0]\n",
    "            ball_speed[1] = ball_speed[1] + random.uniform(-.1, .1) * BALL_SPEED\n",
    "\n",
    "        if ball.top <= 0 or ball.bottom >= HEIGHT:\n",
    "            ball_speed[1] = -ball_speed[1]\n",
    "\n",
    "        # Score update\n",
    "        if ball.left <= 0:\n",
    "            if not ball_scored:\n",
    "                score_player2 += 1\n",
    "                ball_scored = True\n",
    "                if score_player2 == 10:\n",
    "                    game_over = True\n",
    "\n",
    "        elif ball.right >= WIDTH:\n",
    "            if not ball_scored:\n",
    "                score_player1 += 1\n",
    "                ball_scored = True\n",
    "                if score_player1 == 10:\n",
    "                    game_over = True\n",
    "\n",
    "        # Reset the ball when a point is scored\n",
    "        if ball.left <= 0 or ball.right >= WIDTH:\n",
    "            reset_game()\n",
    "\n",
    "        # Reset ball_scored flag when ball crosses the center line\n",
    "        if WIDTH // 2 - 5 < ball.centerx < WIDTH // 2 + 5:\n",
    "            ball_scored = False\n",
    "\n",
    "\n",
    "    # Increase ball speed over time\n",
    "    ball_speed[0] *= 1.0001\n",
    "    ball_speed[1] *= 1.0001\n",
    "\n",
    "    # Draw everything\n",
    "    screen.fill((0, 0, 0))\n",
    "    pygame.draw.rect(screen, WHITE, player1)\n",
    "    pygame.draw.rect(screen, WHITE, player2)\n",
    "\n",
    "    if not game_over:  # Draw the ball only if the game is not over\n",
    "        pygame.draw.ellipse(screen, WHITE, ball)\n",
    "\n",
    "    draw_text(str(score_player1), WIDTH // 4, 50)\n",
    "    draw_text(str(score_player2), WIDTH // 4 * 3, 50)\n",
    "\n",
    "    # Check for winner\n",
    "    if game_over:\n",
    "        draw_text(\"Player 1 Wins!\" if score_player1 == 10 else \"Player 2 Wins!\", WIDTH // 2, HEIGHT // 2)\n",
    "        draw_text(\"Press SPACE to play again\", WIDTH // 2, HEIGHT // 2 + 50)\n",
    "\n",
    "        keys = pygame.key.get_pressed()\n",
    "        if keys[pygame.K_SPACE]:\n",
    "            reset_game()\n",
    "            game_over = False\n",
    "            score_player1 = 0\n",
    "            score_player2 = 0\n",
    "\n",
    "    pygame.display.flip()\n",
    "    clock.tick(FPS)\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-16T19:15:54.793237500Z",
     "start_time": "2023-12-16T19:07:46.915261600Z"
    }
   },
   "id": "88218df0134ce6d2"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
